"""
Railway-optimized clustering cache.

This module provides persistent caching for clustering results using Railway volumes.
Cache hits provide ~600x speedup (60s â†’ 0.1s).
"""

from pathlib import Path
import json
import hashlib
import time
import numpy as np
import pandas as pd
from typing import Optional, Dict, Any
import logging

logger = logging.getLogger(__name__)


class RailwayClusteringCache:
    """
    File-based cache for clustering results, optimized for Railway volumes.
    
    Features:
    - Fast fingerprinting (sample-based hashing)
    - TTL-based expiration
    - Automatic numpy/pandas serialization
    - Cleanup utilities
    """
    
    def __init__(
        self, 
        cache_dir: str = "data/cache/clustering",
        ttl_seconds: int = 86400  # 24 hours
    ):
        """
        Initialize cache.
        
        Args:
            cache_dir: Directory for cache files (Railway volume mount point)
            ttl_seconds: Time-to-live for cache entries (default: 24 hours)
        """
        self.cache_dir = Path(cache_dir)
        self.cache_dir.mkdir(parents=True, exist_ok=True)
        self.ttl = ttl_seconds
        
        logger.info(f"Clustering cache initialized at {self.cache_dir}")
    
    def get_fingerprint(self, df: pd.DataFrame) -> str:
        """
        Generate fast fingerprint for dataset.
        
        Uses sample-based hashing for speed:
        - Shape (rows, columns)
        - Column names and types
        - First/last 100 rows
        - Numeric column means
        
        Args:
            df: Input dataframe
            
        Returns:
            SHA256 hash as hex string
        """
        try:
            characteristics = {
                # Shape
                "shape": df.shape,
                
                # Schema
                "columns": sorted(df.columns.tolist()),
                "dtypes": {col: str(dtype) for col, dtype in df.dtypes.items()},
                
                # Sample data (first/last 100 rows)
                "head_hash": hashlib.md5(
                    df.head(100).to_json(orient="split").encode()
                ).hexdigest(),
                "tail_hash": hashlib.md5(
                    df.tail(100).to_json(orient="split").encode()
                ).hexdigest(),
                
                # Quick statistics
                "numeric_means": {
                    col: float(df[col].mean())
                    for col in df.select_dtypes(include="number").columns[:10]  # First 10 numeric
                },
                "null_counts": df.isnull().sum().head(10).to_dict(),  # First 10 columns
            }
            
            # Hash the characteristics
            fingerprint_str = json.dumps(characteristics, sort_keys=True)
            fingerprint = hashlib.sha256(fingerprint_str.encode()).hexdigest()
            
            logger.debug(f"Generated fingerprint: {fingerprint[:16]}... for shape {df.shape}")
            return fingerprint
            
        except Exception as e:
            logger.error(f"Error generating fingerprint: {e}")
            # Fallback to simple hash
            return hashlib.md5(str(df.shape).encode()).hexdigest()
    
    def get(self, fingerprint: str) -> Optional[Dict[str, Any]]:
        """
        Retrieve cached clustering results.
        
        Args:
            fingerprint: Dataset fingerprint
            
        Returns:
            Cached results dict or None if not found/expired
        """
        cache_file = self.cache_dir / f"{fingerprint}.json"
        
        if not cache_file.exists():
            logger.debug(f"Cache MISS: {fingerprint[:16]}... (file not found)")
            return None
        
        # Check TTL
        age_seconds = time.time() - cache_file.stat().st_mtime
        if age_seconds > self.ttl:
            logger.info(f"Cache EXPIRED: {fingerprint[:16]}... (age: {age_seconds:.0f}s)")
            cache_file.unlink()  # Delete expired cache
            return None
        
        try:
            with open(cache_file, 'r') as f:
                results = json.load(f)
            
            logger.info(f"Cache HIT: {fingerprint[:16]}... (age: {age_seconds:.0f}s)")
            return results
            
        except Exception as e:
            logger.error(f"Error reading cache {fingerprint[:16]}...: {e}")
            return None
    
    def set(self, fingerprint: str, results: Dict[str, Any]) -> bool:
        """
        Store clustering results in cache.
        
        Args:
            fingerprint: Dataset fingerprint
            results: Clustering results to cache
            
        Returns:
            True if successful, False otherwise
        """
        cache_file = self.cache_dir / f"{fingerprint}.json"
        
        try:
            # Convert numpy/pandas types to JSON-serializable
            serializable = self._make_serializable(results)
            
            with open(cache_file, 'w') as f:
                json.dump(serializable, f, indent=2)
            
            logger.info(f"Cache SET: {fingerprint[:16]}... ({cache_file.stat().st_size / 1024:.1f} KB)")
            return True
            
        except Exception as e:
            logger.error(f"Error writing cache {fingerprint[:16]}...: {e}")
            return False
    
    def _make_serializable(self, obj: Any) -> Any:
        """
        Recursively convert numpy/pandas types to Python types.
        
        Args:
            obj: Object to convert
            
        Returns:
            JSON-serializable version
        """
        if isinstance(obj, np.ndarray):
            return obj.tolist()
        elif isinstance(obj, dict):
            return {k: self._make_serializable(v) for k, v in obj.items()}
        elif isinstance(obj, list):
            return [self._make_serializable(item) for item in obj]
        elif isinstance(obj, (np.integer, np.floating)):
            return float(obj)
        elif isinstance(obj, np.bool_):
            return bool(obj)
        elif pd.isna(obj):
            return None
        return obj
    
    def validate(self, fingerprint: str, df: pd.DataFrame) -> bool:
        """
        Validate that cached results are still applicable to dataset.
        
        Args:
            fingerprint: Dataset fingerprint
            df: Current dataframe
            
        Returns:
            True if cache is valid, False otherwise
        """
        cached = self.get(fingerprint)
        if not cached:
            return False
        
        # Check row count (within 10%)
        cached_rows = cached.get("_metadata", {}).get("row_count", 0)
        if cached_rows > 0:
            row_diff = abs(len(df) - cached_rows) / cached_rows
            if row_diff > 0.1:
                logger.warning(f"Cache invalid: row count mismatch ({len(df)} vs {cached_rows})")
                return False
        
        # Check columns
        cached_cols = set(cached.get("_metadata", {}).get("columns", []))
        current_cols = set(df.columns)
        if cached_cols and cached_cols != current_cols:
            logger.warning(f"Cache invalid: column mismatch")
            return False
        
        return True
    
    def cleanup_old(self, max_age_days: int = 7) -> int:
        """
        Remove cache files older than max_age_days.
        
        Args:
            max_age_days: Maximum age in days
            
        Returns:
            Number of files removed
        """
        cutoff = time.time() - (max_age_days * 86400)
        removed = 0
        total_size = 0
        
        for cache_file in self.cache_dir.glob("*.json"):
            if cache_file.stat().st_mtime < cutoff:
                size = cache_file.stat().st_size
                cache_file.unlink()
                removed += 1
                total_size += size
        
        if removed > 0:
            logger.info(f"Cleanup: removed {removed} files ({total_size / 1024 / 1024:.1f} MB)")
        
        return removed
    
    def get_stats(self) -> Dict[str, Any]:
        """
        Get cache statistics.
        
        Returns:
            Dict with cache stats (count, total_size, oldest, newest)
        """
        cache_files = list(self.cache_dir.glob("*.json"))
        
        if not cache_files:
            return {
                "count": 0,
                "total_size_mb": 0,
                "oldest_age_hours": 0,
                "newest_age_hours": 0,
            }
        
        total_size = sum(f.stat().st_size for f in cache_files)
        now = time.time()
        ages = [(now - f.stat().st_mtime) / 3600 for f in cache_files]  # Hours
        
        return {
            "count": len(cache_files),
            "total_size_mb": total_size / 1024 / 1024,
            "oldest_age_hours": max(ages),
            "newest_age_hours": min(ages),
        }


# Singleton instance for easy import
_cache_instance = None

def get_cache() -> RailwayClusteringCache:
    """Get or create singleton cache instance."""
    global _cache_instance
    if _cache_instance is None:
        _cache_instance = RailwayClusteringCache()
    return _cache_instance
